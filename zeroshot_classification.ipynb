{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Vivian\\anaconda3\\envs\\conch\\lib\\site-packages\\timm\\models\\layers\\__init__.py:48: FutureWarning: Importing from timm.models.layers is deprecated, please import via timm.layers\n",
      "  warnings.warn(f\"Importing from {__name__} is deprecated, please import via timm.layers\", FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "from conch.open_clip_custom import create_model_from_pretrained, tokenize, get_tokenizer\n",
    "import torch\n",
    "from torch import nn\n",
    "import os\n",
    "from PIL import Image\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import glob\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import tqdm\n",
    "import skimage\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchvision import transforms\n",
    "import timm\n",
    "from huggingface_hub import login, hf_hub_download\n",
    "\n",
    "\n",
    "# show all jupyter output\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HistopathologyDataset(Dataset):\n",
    "    def __init__(self, csv_file, transform=None):\n",
    "        self.data = pd.read_csv(csv_file)\n",
    "        self.transform = transform\n",
    "        self.label_map = {'FA': 0, 'PT': 1}\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        path = self.data.iloc[idx]['image']\n",
    "        label = self.label_map[self.data.iloc[idx]['class']]\n",
    "        image = np.load(path)\n",
    "        if image.shape[-1] == 3:\n",
    "            image = np.transpose(image, (2, 0, 1))\n",
    "        if image.shape[1:] != (224, 224):\n",
    "            image = skimage.transform.resize(image, (3, 224, 224), anti_aliasing=True)\n",
    "        image = torch.tensor(image, dtype=torch.float32)\n",
    "        return image, label\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UNIEncoderOnly(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.model = timm.create_model(\n",
    "            \"vit_large_patch16_224\",\n",
    "            img_size=224,\n",
    "            patch_size=16,\n",
    "            init_values=1e-5,\n",
    "            num_classes=0,\n",
    "            dynamic_img_size=True,\n",
    "            pretrained=False,\n",
    "        )\n",
    "        checkpoint_path = r\"C:\\Users\\Vivian\\Documents\\CONCH\\checkpoints\\uni\\pytorch_model.bin\"\n",
    "        self.model.load_state_dict(torch.load(checkpoint_path, map_location=\"cpu\"), strict=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_features(dataloader, model, device):\n",
    "    model.eval()\n",
    "    all_features, all_labels = [], []\n",
    "    with torch.no_grad():\n",
    "        for imgs, labels in tqdm.tqdm(dataloader):\n",
    "            imgs = imgs.to(device)\n",
    "            feats = model(imgs)\n",
    "            all_features.append(feats.cpu())\n",
    "            all_labels.extend(labels)\n",
    "    return torch.cat(all_features).numpy(), np.array(all_labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Load model + data\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "encoder = UNIEncoderOnly().to(device)\n",
    "\n",
    "train_loader = DataLoader(HistopathologyDataset(\"train.csv\"), batch_size=32)\n",
    "val_loader = DataLoader(HistopathologyDataset(\"val.csv\"), batch_size=32)\n",
    "test_loader = DataLoader(HistopathologyDataset(\"test.csv\"), batch_size=32)\n",
    "\n",
    "# Feature extraction\n",
    "X_train, y_train = extract_features(train_loader, encoder, device)\n",
    "X_val, y_val = extract_features(val_loader, encoder, device)\n",
    "X_test, y_test = extract_features(test_loader, encoder, device)\n",
    "\n",
    "# Train logistic regression\n",
    "clf = LogisticRegression(max_iter=1000)\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate\n",
    "y_pred = clf.predict(X_test)\n",
    "print(classification_report(y_test, y_pred, target_names=[\"FA\", \"PT\"]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extracting features for Clean lab - using UNI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Vivian\\AppData\\Local\\Temp\\ipykernel_32176\\4132028024.py:50: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  self.model.load_state_dict(torch.load(ckpt, map_location=\"cpu\"), strict=True)\n",
      "Extracting UNI embeddings: 100%|██████████| 16509/16509 [1:54:29<00:00,  2.40it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Saved embeddings to C:\\Users\\Vivian\\Documents\\CONCH\\embeddings\\UNI_test_features.npy with shape (528284, 1024)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tqdm\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import skimage.transform\n",
    "import timm\n",
    "\n",
    "# -----------------------------\n",
    "# Dataset\n",
    "# -----------------------------\n",
    "class PatchDataset(Dataset):\n",
    "    def __init__(self, csv_path, transform=None):\n",
    "        self.df = pd.read_csv(csv_path)\n",
    "        self.label_map = {'FA': 0, 'PT': 1}\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        path = self.df.iloc[idx]['path']\n",
    "        patch = np.load(path)\n",
    "\n",
    "        if patch.shape[-1] == 3:\n",
    "            patch = np.transpose(patch, (2, 0, 1))\n",
    "        if patch.shape[1:] != (224, 224):\n",
    "            patch = skimage.transform.resize(patch, (3, 224, 224), anti_aliasing=True)\n",
    "        patch = torch.tensor(patch, dtype=torch.float32)\n",
    "        label = self.label_map[self.df.iloc[idx]['label']]\n",
    "        return patch, label, path\n",
    "\n",
    "# -----------------------------\n",
    "# UNI Backbone\n",
    "# -----------------------------\n",
    "class UNIEncoderOnly(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.model = timm.create_model(\n",
    "            \"vit_large_patch16_224\",\n",
    "            img_size=224,\n",
    "            patch_size=16,\n",
    "            init_values=1e-5,\n",
    "            num_classes=0,\n",
    "            dynamic_img_size=True,\n",
    "            pretrained=False,\n",
    "        )\n",
    "        ckpt = r\"C:\\Users\\Vivian\\Documents\\CONCH\\checkpoints\\uni\\pytorch_model.bin\"\n",
    "        self.model.load_state_dict(torch.load(ckpt, map_location=\"cpu\"), strict=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "# -----------------------------\n",
    "# Extract + Save\n",
    "# -----------------------------\n",
    "def extract_and_save_embeddings(csv_path, output_path):\n",
    "    dataset = PatchDataset(csv_path)\n",
    "    loader = DataLoader(dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "    encoder = UNIEncoderOnly().to(device)\n",
    "    encoder.eval()\n",
    "\n",
    "    all_embeddings = []\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm.tqdm(loader, desc=\"Extracting UNI embeddings\"):\n",
    "            images, _, _ = batch  # unpack\n",
    "            images = images.to(device)\n",
    "            features = encoder(images)  # [B, 1024]\n",
    "            all_embeddings.append(features.cpu().numpy())\n",
    "\n",
    "    feature_array = np.concatenate(all_embeddings, axis=0)\n",
    "    # ✅ Ensure output directory exists\n",
    "    os.makedirs(os.path.dirname(output_path), exist_ok=True)\n",
    "    \n",
    "    np.save(output_path, feature_array)\n",
    "    print(f\"✅ Saved embeddings to {output_path} with shape {feature_array.shape}\")\n",
    "\n",
    "# -----------------------------\n",
    "# Run\n",
    "# -----------------------------\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "extract_and_save_embeddings(\n",
    "    csv_path=r\"C:\\Users\\Vivian\\Documents\\cleanlab\\patch_metadata.csv\",\n",
    "    output_path=r\"C:\\Users\\Vivian\\Documents\\CONCH\\embeddings\\UNI_test_features.npy\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extracting features for CL - using CONCH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Vivian\\Documents\\CONCH\\conch\\open_clip_custom\\factory.py:18: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(checkpoint_path, map_location=map_location)\n",
      "Extracting CONCH embeddings: 100%|██████████| 16509/16509 [59:20<00:00,  4.64it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Saved embeddings to C:\\Users\\Vivian\\Documents\\CONCH\\embeddings\\CONCH_test_features.npy with shape (528284, 512)\n"
     ]
    }
   ],
   "source": [
    "from conch.open_clip_custom import create_model_from_pretrained\n",
    "\n",
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tqdm\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import skimage.transform\n",
    "import timm\n",
    "\n",
    "# -----------------------------\n",
    "# Dataset\n",
    "# -----------------------------\n",
    "class PatchDataset(Dataset):\n",
    "    def __init__(self, csv_path, transform=None):\n",
    "        self.df = pd.read_csv(csv_path)\n",
    "        self.label_map = {'FA': 0, 'PT': 1}\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        path = self.df.iloc[idx]['path']\n",
    "        patch = np.load(path)\n",
    "\n",
    "        if patch.shape[-1] == 3:\n",
    "            patch = np.transpose(patch, (2, 0, 1))\n",
    "        if patch.shape[1:] != (224, 224):\n",
    "            patch = skimage.transform.resize(patch, (3, 224, 224), anti_aliasing=True)\n",
    "        patch = torch.tensor(patch, dtype=torch.float32)\n",
    "        label = self.label_map[self.df.iloc[idx]['label']]\n",
    "        return patch, label, path\n",
    "\n",
    "# -----------------------------\n",
    "# CONCH Backbone\n",
    "# -----------------------------\n",
    "\n",
    "class CONCHEncoderOnly(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        model_cfg = \"conch_ViT-B-16\"\n",
    "        ckpt_path = r\"C:\\Users\\Vivian\\Documents\\CONCH\\checkpoints\\conch\\pytorch_model.bin\"\n",
    "        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.model, _ = create_model_from_pretrained(model_cfg, ckpt_path, device=device)\n",
    "\n",
    "    def forward(self, x):\n",
    "        feats, _ = self.model.visual(x)  # [B, 512]\n",
    "        return feats\n",
    "\n",
    "# -----------------------------\n",
    "# Extract + Save\n",
    "# -----------------------------\n",
    "def extract_and_save_embeddings(csv_path, output_path):\n",
    "    dataset = PatchDataset(csv_path)\n",
    "    loader = DataLoader(dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "    encoder = CONCHEncoderOnly().to(device)\n",
    "    encoder.eval()\n",
    "\n",
    "    all_embeddings = []\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm.tqdm(loader, desc=\"Extracting CONCH embeddings\"):\n",
    "            images, _, _ = batch  # unpack\n",
    "            images = images.to(device)\n",
    "            features = encoder(images)  # [B, 1024]\n",
    "            all_embeddings.append(features.cpu().numpy())\n",
    "\n",
    "    feature_array = np.concatenate(all_embeddings, axis=0)\n",
    "    # ✅ Ensure output directory exists\n",
    "    os.makedirs(os.path.dirname(output_path), exist_ok=True)\n",
    "    \n",
    "    np.save(output_path, feature_array)\n",
    "    print(f\"✅ Saved embeddings to {output_path} with shape {feature_array.shape}\")\n",
    "\n",
    "# -----------------------------\n",
    "# Run\n",
    "# -----------------------------\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "extract_and_save_embeddings(\n",
    "    csv_path=r\"C:\\Users\\Vivian\\Documents\\cleanlab\\patch_metadata.csv\",\n",
    "    output_path=r\"C:\\Users\\Vivian\\Documents\\CONCH\\embeddings\\CONCH_test_features.npy\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "zeroshot classification?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Vivian\\anaconda3\\envs\\conch\\lib\\site-packages\\timm\\models\\layers\\__init__.py:48: FutureWarning: Importing from timm.models.layers is deprecated, please import via timm.layers\n",
      "  warnings.warn(f\"Importing from {__name__} is deprecated, please import via timm.layers\", FutureWarning)\n",
      "C:\\Users\\Vivian\\AppData\\Local\\Temp\\ipykernel_34848\\1246572078.py:205: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(ckpt_path, map_location=\"cuda\"), strict=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📌 Building class prototypes...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting support embeddings: 100%|██████████| 1/1 [00:00<00:00,  2.04it/s]\n",
      "Classifying test set: 100%|██████████| 3310/3310 [52:19<00:00,  1.05it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📊 Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          FA       0.75      0.49      0.59     81825\n",
      "          PT       0.20      0.44      0.28     24077\n",
      "\n",
      "    accuracy                           0.48    105902\n",
      "   macro avg       0.48      0.46      0.44    105902\n",
      "weighted avg       0.62      0.48      0.52    105902\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tqdm\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import torch.nn.functional as F\n",
    "from sklearn.metrics import classification_report\n",
    "from conch.open_clip_custom import create_model_from_pretrained, tokenize, get_tokenizer\n",
    "\n",
    "\n",
    "# -------------------------------\n",
    "# PatchDataset\n",
    "# -------------------------------\n",
    "class PatchDataset(Dataset):\n",
    "    def __init__(self, csv_path, transform=None):\n",
    "        self.df = pd.read_csv(csv_path)\n",
    "        self.label_map = {'FA': 0, 'PT': 1}\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        path = self.df.iloc[idx]['path']\n",
    "        label = self.label_map[self.df.iloc[idx]['class']]\n",
    "        patch = np.load(path)\n",
    "\n",
    "        if patch.shape[-1] == 3:\n",
    "            patch = np.transpose(patch, (2, 0, 1))\n",
    "\n",
    "        patch = torch.tensor(patch, dtype=torch.float32)\n",
    "        if patch.shape[1:] != (224, 224):\n",
    "            patch = F.interpolate(patch.unsqueeze(0), size=(224, 224), mode='bilinear', align_corners=False).squeeze(0)\n",
    "\n",
    "        return patch, label, path\n",
    "\n",
    "# updated dataset class for our private dataset with numpy files\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "class HistopathologyDataset(Dataset):\n",
    "    def __init__(self, csv_file, transform=None):\n",
    "        \"\"\"\n",
    "        Custom PyTorch Dataset for loading histopathology patches from .npy files.\n",
    "        \n",
    "        Args:\n",
    "            csv_file (str): Path to the dataset metadata CSV file.\n",
    "            transform (callable, optional): Optional transform to be applied on a sample.\n",
    "        \"\"\"\n",
    "        self.data = pd.read_csv(csv_file)\n",
    "        self.transform = transform\n",
    "\n",
    "        # Mapping FA -> 0, PT -> 1\n",
    "        self.label_map = {'FA': 0, 'PT': 1}\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Load image patch\n",
    "        img_path = self.data.iloc[idx]['image']\n",
    "        image = np.load(img_path)  # Load .npy file (already in NumPy format)\n",
    "\n",
    "        # Ensure image is in (C, H, W) format for PyTorch\n",
    "        if image.shape[-1] == 3:  # Check if image is in (H, W, C) format\n",
    "            image = np.transpose(image, (2, 0, 1))  # Convert to (C, H, W)\n",
    "\n",
    "        # Resize to 224x224 if needed\n",
    "        if image.shape[1] != 224 or image.shape[2] != 224:\n",
    "            import skimage.transform\n",
    "            image = skimage.transform.resize(image, (3, 224, 224), anti_aliasing=True)\n",
    "        \n",
    "        # Normalize pixel values\n",
    "        image = torch.tensor(image, dtype=torch.float32)\n",
    "\n",
    "        # Apply transformations if provided\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        # Get label\n",
    "        class_name = self.data.iloc[idx]['class']\n",
    "        label = self.label_map[class_name]  # Convert class name to label\n",
    "\n",
    "        return image, label, img_path\n",
    "        # return image, label\n",
    "\n",
    "# -------------------------------\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import timm\n",
    "from huggingface_hub import login, hf_hub_download\n",
    "\n",
    "# -------------------------------\n",
    "# UNI Model as Feature Extractor\n",
    "# -------------------------------\n",
    "class UNIEncoder(nn.Module):\n",
    "    def __init__(self, checkpoint_path=None):\n",
    "        super().__init__()\n",
    "        self.model = self.make_uni()\n",
    "\n",
    "        # Freeze all parameters\n",
    "        for param in self.model.parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "        if checkpoint_path:\n",
    "            print(f\"Loading checkpoint from: {checkpoint_path}\")\n",
    "            self.model.load_state_dict(torch.load(checkpoint_path, map_location='cuda'), strict=True)\n",
    "\n",
    "    def make_uni(self):\n",
    "        local_dir = r\"C:\\Users\\Vivian\\Documents\\CONCH\\checkpoints\\uni\"  # Your UNI checkpoint path\n",
    "        os.makedirs(local_dir, exist_ok=True)\n",
    "\n",
    "        model = timm.create_model(\n",
    "            \"vit_large_patch16_224\", img_size=224, patch_size=16, init_values=1e-5,\n",
    "            num_classes=0, dynamic_img_size=True\n",
    "        )\n",
    "        model.load_state_dict(\n",
    "            torch.load(os.path.join(local_dir, \"pytorch_model.bin\"), map_location=\"cpu\"),\n",
    "            strict=True\n",
    "        )\n",
    "        return model\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)  # Only returns the encoded features (no classification)\n",
    "    \n",
    "# conch\n",
    "\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from conch.open_clip_custom import create_model_from_pretrained\n",
    "\n",
    "# -------------------------------\n",
    "# CONCH Model as Feature Extractor\n",
    "# -------------------------------\n",
    "class CONCHEncoder(nn.Module):\n",
    "    def __init__(self, model_cfg='conch_ViT-B-16', checkpoint_path=None):\n",
    "        super().__init__()\n",
    "        self.model_cfg = model_cfg\n",
    "        self.checkpoint_path = checkpoint_path or r'C:\\Users\\Vivian\\Documents\\CONCH\\checkpoints\\conch\\pytorch_model.bin'\n",
    "        self.model = self.make_conch()\n",
    "\n",
    "        # Freeze all parameters\n",
    "        for param in self.model.parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "    def make_conch(self):\n",
    "        device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "        model, preprocess = create_model_from_pretrained(\n",
    "            self.model_cfg,\n",
    "            self.checkpoint_path,\n",
    "            device=device\n",
    "        )\n",
    "        return model\n",
    "\n",
    "    def forward(self, x):\n",
    "        out, _ = self.model.visual(x)  # Extract only the image features\n",
    "        return out\n",
    "\n",
    "# -------------------------------\n",
    "# uni2 Model as Feature Extractor\n",
    "# -------------------------------\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import timm\n",
    "\n",
    "class UNI2Encoder(nn.Module):\n",
    "    def __init__(self, checkpoint_path=None):\n",
    "        super().__init__()\n",
    "        self.model = self.make_uni2(checkpoint_path)\n",
    "\n",
    "        # Freeze all parameters\n",
    "        for param in self.model.parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "    def make_uni2(self, checkpoint_path):\n",
    "        local_dir = checkpoint_path or r\"C:\\Users\\Vivian\\Documents\\UNI2\\UNI\\assets\\ckpts\\uni2-h\"\n",
    "        os.makedirs(local_dir, exist_ok=True)\n",
    "\n",
    "        timm_kwargs = {\n",
    "            'model_name': 'vit_giant_patch14_224',\n",
    "            'img_size': 224,\n",
    "            'patch_size': 14,\n",
    "            'depth': 24,\n",
    "            'num_heads': 24,\n",
    "            'init_values': 1e-5,\n",
    "            'embed_dim': 1536,\n",
    "            'mlp_ratio': 2.66667 * 2,\n",
    "            'num_classes': 0,\n",
    "            'no_embed_class': True,\n",
    "            'mlp_layer': timm.layers.SwiGLUPacked,\n",
    "            'act_layer': torch.nn.SiLU,\n",
    "            'reg_tokens': 8,\n",
    "            'dynamic_img_size': True\n",
    "        }\n",
    "\n",
    "        model = timm.create_model(**timm_kwargs)\n",
    "        ckpt_path = os.path.join(local_dir, \"pytorch_model.bin\")\n",
    "        model.load_state_dict(torch.load(ckpt_path, map_location=\"cuda\"), strict=True)\n",
    "\n",
    "        return model\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)  # Output is raw image embedding [B, 1536]\n",
    "\n",
    "\n",
    "# -------------------------------\n",
    "# Load Model for Evaluation\n",
    "# -------------------------------\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "# model = UNIEncoder().to(device)\n",
    "model = UNI2Encoder().to(device)\n",
    "\n",
    "model.eval()\n",
    "\n",
    "\n",
    "# -------------------------------\n",
    "# Load Support Set & Build Prototypes\n",
    "# -------------------------------\n",
    "def build_class_prototypes(support_loader, model):\n",
    "    model.eval()\n",
    "    features, labels = [], []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for images, lbls, _ in tqdm.tqdm(support_loader, desc=\"Extracting support embeddings\"):\n",
    "            images = images.to(device)\n",
    "            # emb = encoder(images)\n",
    "            emb = model(images.to(device))  # [B, 1024]\n",
    "            features.append(emb.cpu())\n",
    "            labels.append(lbls)\n",
    "\n",
    "    features = torch.cat(features)\n",
    "    labels = torch.cat(labels)\n",
    "\n",
    "    class_prototypes = []\n",
    "    for cls in sorted(torch.unique(labels)):\n",
    "        class_feats = features[labels == cls]\n",
    "        proto = class_feats.mean(dim=0)\n",
    "        class_prototypes.append(proto)\n",
    "\n",
    "    return torch.stack(class_prototypes).to(device)  # shape: [num_classes, D]\n",
    "\n",
    "\n",
    "# -------------------------------\n",
    "# Load Test Set\n",
    "# -------------------------------\n",
    "support_csv = \"metadata\\\\patient_split_annotate\\\\patch_csv\\\\ref_features_try1.csv\"  # CSV with few labeled examples per class\n",
    "test_csv = \"metadata\\\\patient_split_annotate\\\\patch_csv\\\\test_patches.csv\"        # CSV with test patches to classify\n",
    "\n",
    "support_loader = DataLoader(HistopathologyDataset(support_csv), batch_size=32, shuffle=False)\n",
    "test_loader = DataLoader(HistopathologyDataset(test_csv), batch_size=32, shuffle=False)\n",
    "\n",
    "# Compute reference features\n",
    "print(\"📌 Building class prototypes...\")\n",
    "prototypes = build_class_prototypes(support_loader, model)\n",
    "\n",
    "\n",
    "# -------------------------------\n",
    "# Predict Test Patches\n",
    "# -------------------------------\n",
    "def predict_with_prototypes(test_loader, model, prototypes):\n",
    "    model.eval()\n",
    "    all_preds, all_labels, all_paths = [], [], []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for images, labels, paths in tqdm.tqdm(test_loader, desc=\"Classifying test set\"):\n",
    "            images = images.to(device)\n",
    "            feats = model(images)\n",
    "            feats = F.normalize(feats, dim=1)\n",
    "            proto_norm = F.normalize(prototypes, dim=1)\n",
    "\n",
    "            sim = feats @ proto_norm.T  # cosine similarity\n",
    "            pred = torch.argmax(sim, dim=1)\n",
    "\n",
    "            all_preds.extend(pred.cpu().tolist())\n",
    "            all_labels.extend(labels.cpu().tolist())\n",
    "            all_paths.extend(paths)\n",
    "\n",
    "    return all_preds, all_labels, all_paths\n",
    "\n",
    "\n",
    "# Run zero-shot classification\n",
    "preds, true_labels, file_paths = predict_with_prototypes(test_loader, model, prototypes)\n",
    "\n",
    "# -------------------------------\n",
    "# Save & Evaluate\n",
    "# -------------------------------\n",
    "df = pd.DataFrame({\n",
    "    \"Patch Path\": file_paths,\n",
    "    \"Predicted\": preds,\n",
    "    \"True Label\": true_labels\n",
    "})\n",
    "df.to_csv(\"uni2_zero_shot_predictions_run1.csv\", index=False)\n",
    "\n",
    "print(\"📊 Classification Report:\")\n",
    "print(classification_report(true_labels, preds, target_names=[\"FA\", \"PT\"]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "conch zs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Vivian\\Documents\\CONCH\\conch\\open_clip_custom\\factory.py:18: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(checkpoint_path, map_location=map_location)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📌 Building class prototypes...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting support embeddings: 100%|██████████| 1/1 [00:00<00:00, 14.26it/s]\n",
      "Classifying test set: 100%|██████████| 3310/3310 [09:51<00:00,  5.59it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📊 Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          FA       0.79      0.68      0.73     81825\n",
      "          PT       0.26      0.39      0.31     24077\n",
      "\n",
      "    accuracy                           0.62    105902\n",
      "   macro avg       0.53      0.54      0.52    105902\n",
      "weighted avg       0.67      0.62      0.64    105902\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tqdm\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import torch.nn.functional as F\n",
    "from sklearn.metrics import classification_report\n",
    "from conch.open_clip_custom import create_model_from_pretrained, tokenize, get_tokenizer\n",
    "\n",
    "\n",
    "# -------------------------------\n",
    "# PatchDataset\n",
    "# -------------------------------\n",
    "class PatchDataset(Dataset):\n",
    "    def __init__(self, csv_path, transform=None):\n",
    "        self.df = pd.read_csv(csv_path)\n",
    "        self.label_map = {'FA': 0, 'PT': 1}\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        path = self.df.iloc[idx]['path']\n",
    "        label = self.label_map[self.df.iloc[idx]['class']]\n",
    "        patch = np.load(path)\n",
    "\n",
    "        if patch.shape[-1] == 3:\n",
    "            patch = np.transpose(patch, (2, 0, 1))\n",
    "\n",
    "        patch = torch.tensor(patch, dtype=torch.float32)\n",
    "        if patch.shape[1:] != (224, 224):\n",
    "            patch = F.interpolate(patch.unsqueeze(0), size=(224, 224), mode='bilinear', align_corners=False).squeeze(0)\n",
    "\n",
    "        return patch, label, path\n",
    "\n",
    "# updated dataset class for our private dataset with numpy files\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "class HistopathologyDataset(Dataset):\n",
    "    def __init__(self, csv_file, transform=None):\n",
    "        \"\"\"\n",
    "        Custom PyTorch Dataset for loading histopathology patches from .npy files.\n",
    "        \n",
    "        Args:\n",
    "            csv_file (str): Path to the dataset metadata CSV file.\n",
    "            transform (callable, optional): Optional transform to be applied on a sample.\n",
    "        \"\"\"\n",
    "        self.data = pd.read_csv(csv_file)\n",
    "        self.transform = transform\n",
    "\n",
    "        # Mapping FA -> 0, PT -> 1\n",
    "        self.label_map = {'FA': 0, 'PT': 1}\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Load image patch\n",
    "        img_path = self.data.iloc[idx]['image']\n",
    "        image = np.load(img_path)  # Load .npy file (already in NumPy format)\n",
    "\n",
    "        # Ensure image is in (C, H, W) format for PyTorch\n",
    "        if image.shape[-1] == 3:  # Check if image is in (H, W, C) format\n",
    "            image = np.transpose(image, (2, 0, 1))  # Convert to (C, H, W)\n",
    "\n",
    "        # Resize to 224x224 if needed\n",
    "        if image.shape[1] != 224 or image.shape[2] != 224:\n",
    "            import skimage.transform\n",
    "            image = skimage.transform.resize(image, (3, 224, 224), anti_aliasing=True)\n",
    "        \n",
    "        # Normalize pixel values\n",
    "        image = torch.tensor(image, dtype=torch.float32)\n",
    "\n",
    "        # Apply transformations if provided\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        # Get label\n",
    "        class_name = self.data.iloc[idx]['class']\n",
    "        label = self.label_map[class_name]  # Convert class name to label\n",
    "\n",
    "        return image, label, img_path\n",
    "        # return image, label\n",
    "\n",
    "# -------------------------------\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import timm\n",
    "from huggingface_hub import login, hf_hub_download\n",
    "\n",
    "# -------------------------------\n",
    "# UNI Model as Feature Extractor\n",
    "# -------------------------------\n",
    "class UNIEncoder(nn.Module):\n",
    "    def __init__(self, checkpoint_path=None):\n",
    "        super().__init__()\n",
    "        self.model = self.make_uni()\n",
    "\n",
    "        # Freeze all parameters\n",
    "        for param in self.model.parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "        if checkpoint_path:\n",
    "            print(f\"Loading checkpoint from: {checkpoint_path}\")\n",
    "            self.model.load_state_dict(torch.load(checkpoint_path, map_location='cuda'), strict=True)\n",
    "\n",
    "    def make_uni(self):\n",
    "        local_dir = r\"C:\\Users\\Vivian\\Documents\\CONCH\\checkpoints\\uni\"  # Your UNI checkpoint path\n",
    "        os.makedirs(local_dir, exist_ok=True)\n",
    "\n",
    "        model = timm.create_model(\n",
    "            \"vit_large_patch16_224\", img_size=224, patch_size=16, init_values=1e-5,\n",
    "            num_classes=0, dynamic_img_size=True\n",
    "        )\n",
    "        model.load_state_dict(\n",
    "            torch.load(os.path.join(local_dir, \"pytorch_model.bin\"), map_location=\"cpu\"),\n",
    "            strict=True\n",
    "        )\n",
    "        return model\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)  # Only returns the encoded features (no classification)\n",
    "    \n",
    "# conch\n",
    "\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from conch.open_clip_custom import create_model_from_pretrained\n",
    "\n",
    "# -------------------------------\n",
    "# CONCH Model as Feature Extractor\n",
    "# -------------------------------\n",
    "class CONCHEncoder(nn.Module):\n",
    "    def __init__(self, model_cfg='conch_ViT-B-16', checkpoint_path=None):\n",
    "        super().__init__()\n",
    "        self.model_cfg = model_cfg\n",
    "        self.checkpoint_path = checkpoint_path or r'C:\\Users\\Vivian\\Documents\\CONCH\\checkpoints\\conch\\pytorch_model.bin'\n",
    "        self.model = self.make_conch()\n",
    "\n",
    "        # Freeze all parameters\n",
    "        for param in self.model.parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "    def make_conch(self):\n",
    "        device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "        model, preprocess = create_model_from_pretrained(\n",
    "            self.model_cfg,\n",
    "            self.checkpoint_path,\n",
    "            device=device\n",
    "        )\n",
    "        return model\n",
    "\n",
    "    # def forward(self, x):\n",
    "    #     out, _ = self.model.visual(x)  # Extract only the image features\n",
    "    #     return out\n",
    "    \n",
    "    def forward(self, x, proj_contrast=False, normalize=False):\n",
    "        return self.model.encode_image(x, proj_contrast=proj_contrast, normalize=normalize)\n",
    "\n",
    "\n",
    "# -------------------------------\n",
    "# Load Model for Evaluation\n",
    "# -------------------------------\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "# model = UNIEncoder().to(device)\n",
    "model = CONCHEncoder().to(device)\n",
    "\n",
    "model.eval()\n",
    "\n",
    "\n",
    "# -------------------------------\n",
    "# Load Support Set & Build Prototypes\n",
    "# -------------------------------\n",
    "def build_class_prototypes(support_loader, model):\n",
    "    model.eval()\n",
    "    features, labels = [], []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for images, lbls, _ in tqdm.tqdm(support_loader, desc=\"Extracting support embeddings\"):\n",
    "            images = images.to(device)\n",
    "            # emb = encoder(images)\n",
    "            emb = model(images.to(device))  # [B, 1024]\n",
    "            features.append(emb.cpu())\n",
    "            labels.append(lbls)\n",
    "\n",
    "    features = torch.cat(features)\n",
    "    labels = torch.cat(labels)\n",
    "\n",
    "    class_prototypes = []\n",
    "    for cls in sorted(torch.unique(labels)):\n",
    "        class_feats = features[labels == cls]\n",
    "        proto = class_feats.mean(dim=0)\n",
    "        class_prototypes.append(proto)\n",
    "\n",
    "    return torch.stack(class_prototypes).to(device)  # shape: [num_classes, D]\n",
    "\n",
    "\n",
    "# -------------------------------\n",
    "# Load Test Set\n",
    "# -------------------------------\n",
    "support_csv = \"metadata\\\\patient_split_annotate\\\\patch_csv\\\\ref_features_try1.csv\"  # CSV with few labeled examples per class\n",
    "test_csv = \"metadata\\\\patient_split_annotate\\\\patch_csv\\\\test_patches.csv\"        # CSV with test patches to classify\n",
    "\n",
    "support_loader = DataLoader(HistopathologyDataset(support_csv), batch_size=32, shuffle=False)\n",
    "test_loader = DataLoader(HistopathologyDataset(test_csv), batch_size=32, shuffle=False)\n",
    "\n",
    "# Compute reference features\n",
    "print(\"📌 Building class prototypes...\")\n",
    "prototypes = build_class_prototypes(support_loader, model)\n",
    "\n",
    "\n",
    "# -------------------------------\n",
    "# Predict Test Patches\n",
    "# -------------------------------\n",
    "def predict_with_prototypes(test_loader, model, prototypes):\n",
    "    model.eval()\n",
    "    all_preds, all_labels, all_paths = [], [], []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for images, labels, paths in tqdm.tqdm(test_loader, desc=\"Classifying test set\"):\n",
    "            images = images.to(device)\n",
    "            # feats = model(images)\n",
    "            feats = model(images.to(device), proj_contrast=False, normalize=False)\n",
    "            feats = F.normalize(feats, dim=1)\n",
    "            proto_norm = F.normalize(prototypes, dim=1)\n",
    "\n",
    "            sim = feats @ proto_norm.T  # cosine similarity\n",
    "            pred = torch.argmax(sim, dim=1)\n",
    "\n",
    "            all_preds.extend(pred.cpu().tolist())\n",
    "            all_labels.extend(labels.cpu().tolist())\n",
    "            all_paths.extend(paths)\n",
    "\n",
    "    return all_preds, all_labels, all_paths\n",
    "\n",
    "\n",
    "# Run zero-shot classification\n",
    "preds, true_labels, file_paths = predict_with_prototypes(test_loader, model, prototypes)\n",
    "\n",
    "# -------------------------------\n",
    "# Save & Evaluate\n",
    "# -------------------------------\n",
    "df = pd.DataFrame({\n",
    "    \"Patch Path\": file_paths,\n",
    "    \"Predicted\": preds,\n",
    "    \"True Label\": true_labels\n",
    "})\n",
    "df.to_csv(\"conch_zero_shot_predictions_run1.csv\", index=False)\n",
    "\n",
    "print(\"📊 Classification Report:\")\n",
    "print(classification_report(true_labels, preds, target_names=[\"FA\", \"PT\"]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "conch zs val set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Vivian\\Documents\\CONCH\\conch\\open_clip_custom\\factory.py:18: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(checkpoint_path, map_location=map_location)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📌 Building class prototypes...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting support embeddings: 100%|██████████| 1/1 [00:00<00:00, 15.84it/s]\n",
      "Classifying test set: 100%|██████████| 3307/3307 [12:10<00:00,  4.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📊 Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          FA       0.50      0.74      0.60     49412\n",
      "          PT       0.61      0.35      0.45     56396\n",
      "\n",
      "    accuracy                           0.54    105808\n",
      "   macro avg       0.56      0.55      0.52    105808\n",
      "weighted avg       0.56      0.54      0.52    105808\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tqdm\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import torch.nn.functional as F\n",
    "from sklearn.metrics import classification_report\n",
    "from conch.open_clip_custom import create_model_from_pretrained, tokenize, get_tokenizer\n",
    "\n",
    "\n",
    "# -------------------------------\n",
    "# PatchDataset\n",
    "# -------------------------------\n",
    "class PatchDataset(Dataset):\n",
    "    def __init__(self, csv_path, transform=None):\n",
    "        self.df = pd.read_csv(csv_path)\n",
    "        self.label_map = {'FA': 0, 'PT': 1}\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        path = self.df.iloc[idx]['path']\n",
    "        label = self.label_map[self.df.iloc[idx]['class']]\n",
    "        patch = np.load(path)\n",
    "\n",
    "        if patch.shape[-1] == 3:\n",
    "            patch = np.transpose(patch, (2, 0, 1))\n",
    "\n",
    "        patch = torch.tensor(patch, dtype=torch.float32)\n",
    "        if patch.shape[1:] != (224, 224):\n",
    "            patch = F.interpolate(patch.unsqueeze(0), size=(224, 224), mode='bilinear', align_corners=False).squeeze(0)\n",
    "\n",
    "        return patch, label, path\n",
    "\n",
    "# updated dataset class for our private dataset with numpy files\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "class HistopathologyDataset(Dataset):\n",
    "    def __init__(self, csv_file, transform=None):\n",
    "        \"\"\"\n",
    "        Custom PyTorch Dataset for loading histopathology patches from .npy files.\n",
    "        \n",
    "        Args:\n",
    "            csv_file (str): Path to the dataset metadata CSV file.\n",
    "            transform (callable, optional): Optional transform to be applied on a sample.\n",
    "        \"\"\"\n",
    "        self.data = pd.read_csv(csv_file)\n",
    "        self.transform = transform\n",
    "\n",
    "        # Mapping FA -> 0, PT -> 1\n",
    "        self.label_map = {'FA': 0, 'PT': 1}\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Load image patch\n",
    "        img_path = self.data.iloc[idx]['image']\n",
    "        image = np.load(img_path)  # Load .npy file (already in NumPy format)\n",
    "\n",
    "        # Ensure image is in (C, H, W) format for PyTorch\n",
    "        if image.shape[-1] == 3:  # Check if image is in (H, W, C) format\n",
    "            image = np.transpose(image, (2, 0, 1))  # Convert to (C, H, W)\n",
    "\n",
    "        # Resize to 224x224 if needed\n",
    "        if image.shape[1] != 224 or image.shape[2] != 224:\n",
    "            import skimage.transform\n",
    "            image = skimage.transform.resize(image, (3, 224, 224), anti_aliasing=True)\n",
    "        \n",
    "        # Normalize pixel values\n",
    "        image = torch.tensor(image, dtype=torch.float32)\n",
    "\n",
    "        # Apply transformations if provided\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        # Get label\n",
    "        class_name = self.data.iloc[idx]['class']\n",
    "        label = self.label_map[class_name]  # Convert class name to label\n",
    "\n",
    "        return image, label, img_path\n",
    "        # return image, label\n",
    "\n",
    "# -------------------------------\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import timm\n",
    "from huggingface_hub import login, hf_hub_download\n",
    "\n",
    "# -------------------------------\n",
    "# UNI Model as Feature Extractor\n",
    "# -------------------------------\n",
    "class UNIEncoder(nn.Module):\n",
    "    def __init__(self, checkpoint_path=None):\n",
    "        super().__init__()\n",
    "        self.model = self.make_uni()\n",
    "\n",
    "        # Freeze all parameters\n",
    "        for param in self.model.parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "        if checkpoint_path:\n",
    "            print(f\"Loading checkpoint from: {checkpoint_path}\")\n",
    "            self.model.load_state_dict(torch.load(checkpoint_path, map_location='cuda'), strict=True)\n",
    "\n",
    "    def make_uni(self):\n",
    "        local_dir = r\"C:\\Users\\Vivian\\Documents\\CONCH\\checkpoints\\uni\"  # Your UNI checkpoint path\n",
    "        os.makedirs(local_dir, exist_ok=True)\n",
    "\n",
    "        model = timm.create_model(\n",
    "            \"vit_large_patch16_224\", img_size=224, patch_size=16, init_values=1e-5,\n",
    "            num_classes=0, dynamic_img_size=True\n",
    "        )\n",
    "        model.load_state_dict(\n",
    "            torch.load(os.path.join(local_dir, \"pytorch_model.bin\"), map_location=\"cpu\"),\n",
    "            strict=True\n",
    "        )\n",
    "        return model\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)  # Only returns the encoded features (no classification)\n",
    "    \n",
    "# conch\n",
    "\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from conch.open_clip_custom import create_model_from_pretrained\n",
    "\n",
    "# -------------------------------\n",
    "# CONCH Model as Feature Extractor\n",
    "# -------------------------------\n",
    "class CONCHEncoder(nn.Module):\n",
    "    def __init__(self, model_cfg='conch_ViT-B-16', checkpoint_path=None):\n",
    "        super().__init__()\n",
    "        self.model_cfg = model_cfg\n",
    "        self.checkpoint_path = checkpoint_path or r'C:\\Users\\Vivian\\Documents\\CONCH\\checkpoints\\conch\\pytorch_model.bin'\n",
    "        self.model = self.make_conch()\n",
    "\n",
    "        # Freeze all parameters\n",
    "        for param in self.model.parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "    def make_conch(self):\n",
    "        device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "        model, preprocess = create_model_from_pretrained(\n",
    "            self.model_cfg,\n",
    "            self.checkpoint_path,\n",
    "            device=device\n",
    "        )\n",
    "        return model\n",
    "\n",
    "    # def forward(self, x):\n",
    "    #     out, _ = self.model.visual(x)  # Extract only the image features\n",
    "    #     return out\n",
    "    \n",
    "    def forward(self, x, proj_contrast=False, normalize=False):\n",
    "        return self.model.encode_image(x, proj_contrast=proj_contrast, normalize=normalize)\n",
    "\n",
    "\n",
    "# -------------------------------\n",
    "# Load Model for Evaluation\n",
    "# -------------------------------\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "# model = UNIEncoder().to(device)\n",
    "model = CONCHEncoder().to(device)\n",
    "\n",
    "model.eval()\n",
    "\n",
    "\n",
    "# -------------------------------\n",
    "# Load Support Set & Build Prototypes\n",
    "# -------------------------------\n",
    "def build_class_prototypes(support_loader, model):\n",
    "    model.eval()\n",
    "    features, labels = [], []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for images, lbls, _ in tqdm.tqdm(support_loader, desc=\"Extracting support embeddings\"):\n",
    "            images = images.to(device)\n",
    "            # emb = encoder(images)\n",
    "            emb = model(images.to(device))  # [B, 1024]\n",
    "            features.append(emb.cpu())\n",
    "            labels.append(lbls)\n",
    "\n",
    "    features = torch.cat(features)\n",
    "    labels = torch.cat(labels)\n",
    "\n",
    "    class_prototypes = []\n",
    "    for cls in sorted(torch.unique(labels)):\n",
    "        class_feats = features[labels == cls]\n",
    "        proto = class_feats.mean(dim=0)\n",
    "        class_prototypes.append(proto)\n",
    "\n",
    "    return torch.stack(class_prototypes).to(device)  # shape: [num_classes, D]\n",
    "\n",
    "\n",
    "# -------------------------------\n",
    "# Load Test Set\n",
    "# -------------------------------\n",
    "support_csv = \"metadata\\\\patient_split_annotate\\\\patch_csv\\\\ref_features_try1.csv\"  # CSV with few labeled examples per class\n",
    "test_csv = r\"C:\\Users\\Vivian\\Documents\\CONCH\\metadata\\patient_split_annotate\\patch_csv\\val_patches.csv\"        # CSV with test patches to classify\n",
    "\n",
    "support_loader = DataLoader(HistopathologyDataset(support_csv), batch_size=32, shuffle=False)\n",
    "test_loader = DataLoader(HistopathologyDataset(test_csv), batch_size=32, shuffle=False)\n",
    "\n",
    "# Compute reference features\n",
    "print(\"📌 Building class prototypes...\")\n",
    "prototypes = build_class_prototypes(support_loader, model)\n",
    "\n",
    "\n",
    "# -------------------------------\n",
    "# Predict Test Patches\n",
    "# -------------------------------\n",
    "def predict_with_prototypes(test_loader, model, prototypes):\n",
    "    model.eval()\n",
    "    all_preds, all_labels, all_paths = [], [], []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for images, labels, paths in tqdm.tqdm(test_loader, desc=\"Classifying test set\"):\n",
    "            images = images.to(device)\n",
    "            # feats = model(images)\n",
    "            feats = model(images.to(device), proj_contrast=False, normalize=False)\n",
    "            feats = F.normalize(feats, dim=1)\n",
    "            proto_norm = F.normalize(prototypes, dim=1)\n",
    "\n",
    "            sim = feats @ proto_norm.T  # cosine similarity\n",
    "            pred = torch.argmax(sim, dim=1)\n",
    "\n",
    "            all_preds.extend(pred.cpu().tolist())\n",
    "            all_labels.extend(labels.cpu().tolist())\n",
    "            all_paths.extend(paths)\n",
    "\n",
    "    return all_preds, all_labels, all_paths\n",
    "\n",
    "\n",
    "# Run zero-shot classification\n",
    "preds, true_labels, file_paths = predict_with_prototypes(test_loader, model, prototypes)\n",
    "\n",
    "# -------------------------------\n",
    "# Save & Evaluate\n",
    "# -------------------------------\n",
    "df = pd.DataFrame({\n",
    "    \"Patch Path\": file_paths,\n",
    "    \"Predicted\": preds,\n",
    "    \"True Label\": true_labels\n",
    "})\n",
    "df.to_csv(\"conch_val_zero_shot_predictions_run1.csv\", index=False)\n",
    "\n",
    "print(\"📊 Classification Report:\")\n",
    "print(classification_report(true_labels, preds, target_names=[\"FA\", \"PT\"]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LR classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Load embeddings and labels\n",
    "X = np.load(\"C:/Users/Vivian/Documents/CONCH/embeddings/UNI_test_features.npy\")  # shape [N, 1024]\n",
    "y = np.load(\"C:/Users/Vivian/Documents/CONCH/embeddings/labels.npy\")             # shape [N,]\n",
    "\n",
    "print(\"Loaded:\", X.shape, y.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional: Train/Val Split\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, stratify=y, random_state=42)\n",
    "\n",
    "# Train classifier\n",
    "clf = LogisticRegression(max_iter=1000, class_weight='balanced')  # 'balanced' helps if you have imbalance\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# Predict\n",
    "y_pred = clf.predict(X_val)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(y_val, y_pred, target_names=[\"FA\", \"PT\"]))\n",
    "\n",
    "print(\"Confusion Matrix:\")\n",
    "print(confusion_matrix(y_val, y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = np.load(\"UNI_test_features.npy\")\n",
    "y_test = np.load(\"test_labels.npy\")\n",
    "\n",
    "y_pred = clf.predict(X_test)\n",
    "print(classification_report(y_test, y_pred, target_names=[\"FA\", \"PT\"]))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
